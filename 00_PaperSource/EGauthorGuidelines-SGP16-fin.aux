\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{sketchbasedmodelingsurveycg2009}
\citation{DeckerChildren1988}
\citation{sketchbasedcompositionfunkhousersbim2008,sketchtodesignxukaicgf2013}
\citation{EitRicBouHilAle12}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{brf}{\backcite{sketchbasedmodelingsurveycg2009}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{DeckerChildren1988}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{sketchbasedcompositionfunkhousersbim2008,sketchtodesignxukaicgf2013}{{1}{1}{section.1}}}
\@writefile{brf}{\backcite{EitRicBouHilAle12}{{1}{1}{section.1}}}
\citation{asurveyofcontentbasedtangeldermultimedia2008}
\citation{asearchenginefunkhousertog2003,discriminativesketchbasedshaotianjiacgf2011,EitRicBouHilAle12}
\citation{sketchbasedcompositionfunkhousersbim2008}
\citation{sketchtodesignxukaicgf2013}
\citation{surveyonmeshsegmentationshamircgf2008,benchmarkforsegmentationchenxiaobaisg2009}
\citation{learning3dmeshsegmentationkalosg2010,cosegmentationof3dshapesliuligangcgf2012,jointshapesegmentationhuangqixingsg2011,SidKaiKleZhaCoh11}
\citation{FanMeshCutting2012}
\citation{LeeExampleGalleries2010}
\citation{datadrivenvladenaisa2010}
\citation{probabilisticreasoningvladlensg2011}
\citation{modelingbyexamplefunkhousersg2004}
\citation{suggestivecontoursrusinkiewicztog2003}
\citation{RenLCM2003}
\citation{surveyonmeshsegmentationshamircgf2008}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Given a roughly drawn sketch, our algorithm rapidly searches a shape database to identify and extract customized parts that match the sketch. No presegmentation is required: the retrieved parts can be irregular and not match any standard segmentation.}}{2}{figure.1}}
\newlabel{fig:HeadPic}{{1}{2}{Given a roughly drawn sketch, our algorithm rapidly searches a shape database to identify and extract customized parts that match the sketch. No presegmentation is required: the retrieved parts can be irregular and not match any standard segmentation}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}}
\@writefile{brf}{\backcite{asurveyofcontentbasedtangeldermultimedia2008}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{asearchenginefunkhousertog2003,discriminativesketchbasedshaotianjiacgf2011,EitRicBouHilAle12}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{sketchbasedcompositionfunkhousersbim2008}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{sketchtodesignxukaicgf2013}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{surveyonmeshsegmentationshamircgf2008,benchmarkforsegmentationchenxiaobaisg2009}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{learning3dmeshsegmentationkalosg2010,cosegmentationof3dshapesliuligangcgf2012,jointshapesegmentationhuangqixingsg2011,SidKaiKleZhaCoh11}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{FanMeshCutting2012}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{LeeExampleGalleries2010}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{datadrivenvladenaisa2010}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{probabilisticreasoningvladlensg2011}{{2}{2}{section.2}}}
\@writefile{brf}{\backcite{modelingbyexamplefunkhousersg2004}{{2}{2}{section.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Overview}{2}{section.3}}
\@writefile{brf}{\backcite{suggestivecontoursrusinkiewicztog2003}{{2}{3}{section.3}}}
\@writefile{brf}{\backcite{RenLCM2003}{{2}{3}{section.3}}}
\citation{Igarashi:1999:TSI:311535.311602}
\citation{onvisualsimilaritychencgf2003}
\citation{scalablenearestmujapami2014}
\citation{nearoptimalandoniacm2008}
\citation{scalableknnjingwangcvpr2012}
\citation{scalableknnjingwangcvpr2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Pipeline of our system. In the offline phase, for each model in a given shape database (a), we first extract its boundary contours (b). We then construct the \textbf  {super-face graph} for each model (c), and organize the boundary contours of all models into a \textbf  {randomized compound $k$-NN graph} (d). In the online phase, the user draws a rough sketch to convey his/her design intent (e). Regions of database shapes matching the sketch are rapidly identified via partial shape matching (f). A selected part is further refined to optimize its boundary and better match the sketch (g)}}{3}{figure.2}}
\newlabel{fig:pipeline}{{2}{3}{Pipeline of our system. In the offline phase, for each model in a given shape database (a), we first extract its boundary contours (b). We then construct the \textbf {super-face graph} for each model (c), and organize the boundary contours of all models into a \textbf {randomized compound $k$-NN graph} (d). In the online phase, the user draws a rough sketch to convey his/her design intent (e). Regions of database shapes matching the sketch are rapidly identified via partial shape matching (f). A selected part is further refined to optimize its boundary and better match the sketch (g)\relax }{figure.2}{}}
\@writefile{brf}{\backcite{surveyonmeshsegmentationshamircgf2008}{{3}{3}{section.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Data Structure for Fast Partial Matching}{3}{section.4}}
\newlabel{sec:acc}{{4}{3}{Data Structure for Fast Partial Matching\relax }{section.4}{}}
\@writefile{brf}{\backcite{Igarashi:1999:TSI:311535.311602}{{3}{4}{section.4}}}
\@writefile{brf}{\backcite{onvisualsimilaritychencgf2003}{{3}{4}{section.4}}}
\@writefile{brf}{\backcite{scalablenearestmujapami2014}{{3}{4}{figure.3}}}
\@writefile{brf}{\backcite{nearoptimalandoniacm2008}{{3}{4}{figure.3}}}
\@writefile{brf}{\backcite{scalableknnjingwangcvpr2012}{{3}{4}{figure.3}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A single contour may match a query contour in more than one section. The shape contour (b) matches the query contour (a) in four different sections (marked in red).}}{3}{figure.3}}
\newlabel{fig:CtourMatch}{{3}{3}{A single contour may match a query contour in more than one section. The shape contour (b) matches the query contour (a) in four different sections (marked in red)}{figure.3}{}}
\@writefile{brf}{\backcite{scalableknnjingwangcvpr2012}{{3}{4}{figure.3}}}
\citation{scalableknnjingwangcvpr2012}
\citation{partialedgecontourriemeneccv2010}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Illustration of our randomized compound $k$NN graph. In this figure, a solid circle represents a valid section and a dashed circle represents an invalid one. Given a contour (taken as a node of our randomized compound $k$NN graph), we first generate several sections (a). Then, we find the nearest neighbors for each section and establish edges between the parent contours of these sections (b). Finally, we find valid sections and cluster them (c).}}{4}{figure.4}}
\newlabel{fig:KNN}{{4}{4}{Illustration of our randomized compound $k$NN graph. In this figure, a solid circle represents a valid section and a dashed circle represents an invalid one. Given a contour (taken as a node of our randomized compound $k$NN graph), we first generate several sections (a). Then, we find the nearest neighbors for each section and establish edges between the parent contours of these sections (b). Finally, we find valid sections and cluster them (c)}{figure.4}{}}
\@writefile{brf}{\backcite{scalableknnjingwangcvpr2012}{{4}{2.}{Item.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Candidate Shape Retrieval}{4}{section.5}}
\newlabel{sec:candshape}{{5}{4}{Candidate Shape Retrieval\relax }{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Contour Descriptor}{4}{subsection.5.1}}
\newlabel{subsec:CtourDesc}{{5.1}{4}{Contour Descriptor\relax }{subsection.5.1}{}}
\@writefile{brf}{\backcite{partialedgecontourriemeneccv2010}{{4}{5.1}{subsection.5.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Shape Retrieval}{4}{subsection.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Progressive Part Extraction}{4}{section.6}}
\newlabel{sec:partextraction}{{6}{4}{Progressive Part Extraction\relax }{section.6}{}}
\citation{jointshapesegmentationhuangqixingsg2011}
\citation{randomizedcutsfunkhousertog2008}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Different types of parts (red) retrieved by a sketch (blue) using our algorithm. While some parts may be predefined with an automatic segmentation algorithm (d), others are irregular, defined only by the sketch (a,e), and yet others combine multiple ``standard'' segments (b: shin + foot, c: head + neck, f: shade + neck).}}{5}{figure.5}}
\newlabel{fig:partseg}{{5}{5}{Different types of parts (red) retrieved by a sketch (blue) using our algorithm. While some parts may be predefined with an automatic segmentation algorithm (d), others are irregular, defined only by the sketch (a,e), and yet others combine multiple ``standard'' segments (b: shin + foot, c: head + neck, f: shade + neck)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The construction of the super-face graph for a 3D shape. Given the Teddy model (a), we partition it into a large number of super-faces (b). Each super-face is a node of the graph. Adjacent super-faces are connected by a weighted graph edge (c).}}{5}{figure.6}}
\newlabel{fig:SFG}{{6}{5}{The construction of the super-face graph for a 3D shape. Given the Teddy model (a), we partition it into a large number of super-faces (b). Each super-face is a node of the graph. Adjacent super-faces are connected by a weighted graph edge (c)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Super-Face Graph Representation}{5}{subsection.6.1}}
\newlabel{subsec:sfg}{{6.1}{5}{Super-Face Graph Representation\relax }{subsection.6.1}{}}
\@writefile{brf}{\backcite{jointshapesegmentationhuangqixingsg2011}{{5}{6.1}{subsection.6.1}}}
\@writefile{brf}{\backcite{randomizedcutsfunkhousertog2008}{{5}{6.1}{figure.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Distributions of super-faces on segments. $s_0$, $s_1$, and $s_2$ are the super-faces. $g_0$, $g_1$, and $g_2$ are the segments generated by the randomized segmentation on the shape.}}{5}{figure.7}}
\newlabel{fig:Consistency}{{7}{5}{Distributions of super-faces on segments. $s_0$, $s_1$, and $s_2$ are the super-faces. $g_0$, $g_1$, and $g_2$ are the segments generated by the randomized segmentation on the shape}{figure.7}{}}
\citation{concavityawareyouyitvcg2012}
\citation{anexperimentalcomparisonboykovpami2004}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Illustrations of the complexity of contours and their relationship. Depth-discontinuous contour sections are shown in (a). Topologically different contour sections are illustrated in (b). Isolated contour sections are shown in (c).}}{6}{figure.8}}
\newlabel{fig:ComplexCtour}{{8}{6}{Illustrations of the complexity of contours and their relationship. Depth-discontinuous contour sections are shown in (a). Topologically different contour sections are illustrated in (b). Isolated contour sections are shown in (c)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Fuzzy Part Identification}{6}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Coarse-to-Fine Boundary Refinement}{6}{subsection.6.3}}
\newlabel{sec:partrefinement}{{6.3}{6}{Coarse-to-Fine Boundary Refinement\relax }{subsection.6.3}{}}
\@writefile{brf}{\backcite{concavityawareyouyitvcg2012}{{6}{6.3}{subsection.6.3}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Coarse Level Extraction}{6}{subsubsection.6.3.1}}
\@writefile{brf}{\backcite{anexperimentalcomparisonboykovpami2004}{{6}{6.3.1}{subsubsection.6.3.1}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Fine Level Extraction}{6}{subsubsection.6.3.2}}
\newlabel{sec:finelevelextraction}{{6.3.2}{6}{Fine Level Extraction\relax }{subsubsection.6.3.2}{}}
\citation{hierarchicalmeshdecompositionayellettog2003}
\citation{CGF:CGF947}
\citation{sketchbasedcompositionfunkhousersbim2008}
\citation{KunXu2013}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Examples of sketch-driven assembly-based modeling. In each subfigure, we show the designed models (red), the user's sketches (blue), and the part suggestions selected by the user.}}{7}{figure.9}}
\newlabel{fig:CTModels}{{9}{7}{Examples of sketch-driven assembly-based modeling. In each subfigure, we show the designed models (red), the user's sketches (blue), and the part suggestions selected by the user}{figure.9}{}}
\@writefile{brf}{\backcite{hierarchicalmeshdecompositionayellettog2003}{{7}{6.3.2}{subsubsection.6.3.2}}}
\@writefile{brf}{\backcite{CGF:CGF947}{{7}{6.3.2}{subsubsection.6.3.2}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Applications}{7}{section.7}}
\newlabel{sec:apps}{{7}{7}{Applications\relax }{section.7}{}}
\@writefile{brf}{\backcite{sketchbasedcompositionfunkhousersbim2008}{{7}{7}{section.7}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Example of contour-driven shape completion. Given a photograph of an occluded lamp, the user traces the outer boundary of the visible portion. This trace is used as the query contour. Our algorithm retrieves possible completions for the object from a database of lamps. (Photo: Xavier Young)}}{7}{figure.10}}
\newlabel{fig:Sketch2Scene}{{10}{7}{Example of contour-driven shape completion. Given a photograph of an occluded lamp, the user traces the outer boundary of the visible portion. This trace is used as the query contour. Our algorithm retrieves possible completions for the object from a database of lamps. (Photo: Xavier Young)\relax }{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Example of image-driven part-based modeling. Given the image (on the top left corner), the user sketches the contours of the parts of the shape. Our method extract parts from the database. The user selects the parts and assembles the 3D shape (middle) in the image. The body part of the generated shape is segmented from an existing horse shape.}}{7}{figure.11}}
\newlabel{fig:ImageModeling}{{11}{7}{Example of image-driven part-based modeling. Given the image (on the top left corner), the user sketches the contours of the parts of the shape. Our method extract parts from the database. The user selects the parts and assembles the 3D shape (middle) in the image. The body part of the generated shape is segmented from an existing horse shape}{figure.11}{}}
\@writefile{brf}{\backcite{KunXu2013}{{7}{7}{figure.10}}}
\citation{datadrivenvladenaisa2010,probabilisticreasoningvladlensg2011}
\citation{randomizedcutsfunkhousertog2008}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Examples of shape variations created with our method. Given a segmented model (left), we can use one part as the query to retrieve similar parts from the database. The selected retrieved part is composited with the non-query parts to create a new shape (right) with the same layout as the original shape.}}{8}{figure.12}}
\newlabel{fig:SegedAsInput}{{12}{8}{Examples of shape variations created with our method. Given a segmented model (left), we can use one part as the query to retrieve similar parts from the database. The selected retrieved part is composited with the non-query parts to create a new shape (right) with the same layout as the original shape}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Example of symmetry-aware selection and editing. The user selects an arbitrary region of a shape (a). Note that the region is not the complete wing, i.e. it is not a ``standard'' segment. We use our algorithm as a fast partial matching technique, to find other parts in the same shape with the same contour. This yields the corresponding region of the other wing as a symmetric counterpart (b). Finally, the symmetric pair may be replaced by another, retrieved from a database using our ``shape variations'' approach (c).}}{8}{figure.13}}
\newlabel{fig:SymSel}{{13}{8}{Example of symmetry-aware selection and editing. The user selects an arbitrary region of a shape (a). Note that the region is not the complete wing, i.e. it is not a ``standard'' segment. We use our algorithm as a fast partial matching technique, to find other parts in the same shape with the same contour. This yields the corresponding region of the other wing as a symmetric counterpart (b). Finally, the symmetric pair may be replaced by another, retrieved from a database using our ``shape variations'' approach (c)}{figure.13}{}}
\@writefile{brf}{\backcite{datadrivenvladenaisa2010,probabilisticreasoningvladlensg2011}{{8}{7}{figure.13}}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Examples of part suggestions. Given the query shapes (in green), our system presents a list of part suggestions (in purple). The assembled shapes are shown below the suggested parts.}}{8}{figure.14}}
\newlabel{fig:appl}{{14}{8}{Examples of part suggestions. Given the query shapes (in green), our system presents a list of part suggestions (in purple). The assembled shapes are shown below the suggested parts}{figure.14}{}}
\@writefile{brf}{\backcite{randomizedcutsfunkhousertog2008}{{8}{7}{figure.14}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Evaluation}{8}{section.8}}
\citation{scalableknnjingwangcvpr2012}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces An example of multi-scale part suggestion. The initial shape and subsequent working models, used as queries, are listed in the left column. On the right, we show the suggested parts (red) at different scales for each modeling step. The parts corresponding to the queries are marked in light blue. The other parts are marked in light green.}}{9}{figure.15}}
\newlabel{fig:MSPS}{{15}{9}{An example of multi-scale part suggestion. The initial shape and subsequent working models, used as queries, are listed in the left column. On the right, we show the suggested parts (red) at different scales for each modeling step. The parts corresponding to the queries are marked in light blue. The other parts are marked in light green}{figure.15}{}}
\@writefile{brf}{\backcite{scalableknnjingwangcvpr2012}{{9}{2.}{Item.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Retrieval results generated with our method. The retrieved parts are ranked from top to bottom, from left to right.}}{9}{figure.16}}
\newlabel{fig:MoreRetrievalRes}{{16}{9}{Retrieval results generated with our method. The retrieved parts are ranked from top to bottom, from left to right}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Retrieved parts generated with different methods: the brute force $k$NNG method (a), the Wang's randomized $k$NNG approximation method (b), the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with the brute force method (c), and the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with Wang's method (d).}}{10}{figure.17}}
\newlabel{fig:RCKNNGComp}{{17}{10}{Retrieved parts generated with different methods: the brute force $k$NNG method (a), the Wang's randomized $k$NNG approximation method (b), the {\RCKNNG } with the brute force method (c), and the {\RCKNNG } with Wang's method (d)}{figure.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Retrieval performance for different methods on our test dataset. RT is the retrieval time for a query contour. CT is the construction time for the data structures. ME is the matching error for the retrieved parts. BF $K$NNG represents the Brute force $k$NNG method. Wang $k$NNG represents the Wang's $k$NNG approximation method. BF {\unhbox \voidb@x \hbox {RC-$k$NNG}} represents the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with the brute force method. Wang {\unhbox \voidb@x \hbox {RC-$k$NNG}} represents the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with Wang's method.}}{10}{table.1}}
\newlabel{tab:RCKNNGComp}{{1}{10}{Retrieval performance for different methods on our test dataset. RT is the retrieval time for a query contour. CT is the construction time for the data structures. ME is the matching error for the retrieved parts. BF $K$NNG represents the Brute force $k$NNG method. Wang $k$NNG represents the Wang's $k$NNG approximation method. BF {\RCKNNG } represents the {\RCKNNG } with the brute force method. Wang {\RCKNNG } represents the {\RCKNNG } with Wang's method}{table.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Averaged precision recall curves. (a) shows the precision recall curves generated with different retrieval methods on a set of 35 test sketches. (b) shows the precision recall curves generated with our method under different camera view settings. In (a), BF kNNG represents the Brute force $k$NNG method; Wang kNNG represents the Wang's $k$NNG approximation method; BF RC-kNNG represents represents the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with the brute force method; Wang RC-kNNG represents the {\unhbox \voidb@x \hbox {RC-$k$NNG}} with Wang's method (our method).}}{10}{figure.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Different retrieval methods}}}{10}{figure.18}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Different camera view settings}}}{10}{figure.18}}
\newlabel{fig:PreRecCurve}{{18}{10}{Averaged precision recall curves. (a) shows the precision recall curves generated with different retrieval methods on a set of 35 test sketches. (b) shows the precision recall curves generated with our method under different camera view settings. In (a), BF kNNG represents the Brute force $k$NNG method; Wang kNNG represents the Wang's $k$NNG approximation method; BF RC-kNNG represents represents the {\RCKNNG } with the brute force method; Wang RC-kNNG represents the {\RCKNNG } with Wang's method (our method)}{figure.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Timing statistics with different super-face counts. Full part extraction measures the time from launch of a query to generation of a final part.}}{10}{table.2}}
\newlabel{tab:SFCounts}{{2}{10}{Timing statistics with different super-face counts. Full part extraction measures the time from launch of a query to generation of a final part}{table.2}{}}
\citation{FanWang2013}
\citation{frompartialshapematchingcvpr}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Matching error statistics with different super-face counts, averaged over a set of 20 test sketches.}}{11}{table.3}}
\newlabel{tab:MatchError}{{3}{11}{Matching error statistics with different super-face counts, averaged over a set of 20 test sketches}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Improvement in part quality with increasing super-face count (N). With more super-faces, the extracted part fits the user's sketch better and better.}}{11}{figure.19}}
\newlabel{fig:SFCountsPartQuality}{{19}{11}{Improvement in part quality with increasing super-face count (N). With more super-faces, the extracted part fits the user's sketch better and better}{figure.19}{}}
\@writefile{brf}{\backcite{FanWang2013}{{11}{8}{figure.19}}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Retrieval results generated with our method and the pre-segmentation method. Given the user's sketch, we show the retrieval results generated with the two methods. In (b) and (d), results generated with the pre-segmentation method are on the upper row; results generated with our method are on the lower row. The pre-segmentation method retrieves the predefined parts (the head of the camel model in (b) and the leg of the cow model in (d)), which are most similar to the user's sketch. Our method extracts parts (the ``head+leg'' part of the camel model in (b) and the lower leg of the cow model in (d)) on-the-fly according to the user's sketch.}}{11}{figure.20}}
\newlabel{fig:Comp2PreSeg}{{20}{11}{Retrieval results generated with our method and the pre-segmentation method. Given the user's sketch, we show the retrieval results generated with the two methods. In (b) and (d), results generated with the pre-segmentation method are on the upper row; results generated with our method are on the lower row. The pre-segmentation method retrieves the predefined parts (the head of the camel model in (b) and the leg of the cow model in (d)), which are most similar to the user's sketch. Our method extracts parts (the ``head+leg'' part of the camel model in (b) and the lower leg of the cow model in (d)) on-the-fly according to the user's sketch}{figure.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{11}{section.9}}
\@writefile{brf}{\backcite{frompartialshapematchingcvpr}{{11}{9}{section.9}}}
\bibdata{Bib}
\bibcite{nearoptimalandoniacm2008}{AI08}
\bibcite{concavityawareyouyitvcg2012}{AZC{$^{*}$}12}
\bibcite{anexperimentalcomparisonboykovpami2004}{BK04}
\bibcite{benchmarkforsegmentationchenxiaobaisg2009}{CGF09}
\bibcite{datadrivenvladenaisa2010}{CK10}
\bibcite{probabilisticreasoningvladlensg2011}{CKGK11}
\bibcite{onvisualsimilaritychencgf2003}{CTSO03}
\bibcite{DeckerChildren1988}{Dec88}
\bibcite{suggestivecontoursrusinkiewicztog2003}{DFRS03}
\bibcite{EitRicBouHilAle12}{ERB{$^{*}$}12}
\bibcite{modelingbyexamplefunkhousersg2004}{FKS{$^{*}$}04}
\bibcite{asearchenginefunkhousertog2003}{FMK{$^{*}$}03}
\bibcite{FanMeshCutting2012}{FML12}
\bibcite{FanWang2013}{FWX{$^{*}$}13}
\bibcite{randomizedcutsfunkhousertog2008}{GF08}
\bibcite{cosegmentationof3dshapesliuligangcgf2012}{HFL}
\bibcite{jointshapesegmentationhuangqixingsg2011}{HKG11}
\bibcite{Igarashi:1999:TSI:311535.311602}{IMT99}
\bibcite{CGF:CGF947}{JLCW06}
\bibcite{learning3dmeshsegmentationkalosg2010}{KHS10}
\bibcite{hierarchicalmeshdecompositionayellettog2003}{KT03}
\bibcite{sketchbasedcompositionfunkhousersbim2008}{LF08}
\bibcite{LeeExampleGalleries2010}{LSK{$^{*}$}10}
\bibcite{frompartialshapematchingcvpr}{ML11}
\bibcite{scalablenearestmujapami2014}{ML14}
\bibcite{sketchbasedmodelingsurveycg2009}{OSSJ09}
\bibcite{partialedgecontourriemeneccv2010}{RDB10}
\bibcite{RenLCM2003}{RM03}
\bibcite{surveyonmeshsegmentationshamircgf2008}{Sha08}
\bibcite{SidKaiKleZhaCoh11}{SvKK{$^{*}$}11}
\bibcite{discriminativesketchbasedshaotianjiacgf2011}{SXY{$^{*}$}11}
\bibcite{asurveyofcontentbasedtangeldermultimedia2008}{TV08}
\bibcite{scalableknnjingwangcvpr2012}{WWZ{$^{*}$}12}
\bibcite{KunXu2013}{XCF{$^{*}$}13}
\bibcite{sketchtodesignxukaicgf2013}{XXM{$^{*}$}13}
\bibstyle{eg-alpha-doi}
\@writefile{toc}{\contentsline {section}{References}{12}{section.9}}
